# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZk45bDKAdrvSeerdi7My1IUKsxXVSK8
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install mediapipe tensorflow opencv-python numpy

import cv2
import mediapipe as mp
import numpy as np
from tensorflow.keras.models import load_model

import numpy as np

# âœ… Your sign categories
actions = np.array(['Good Morning', 'Give', 'Crocodile', 'Maybe', 'Knife'])

# Save this list so we use the same one later (important!)
np.save("/content/drive/MyDrive/isl_models/actions.npy", actions)

print("ðŸŽ¯ Actions saved for reference:", actions)

mp_holistic = mp.solutions.holistic

def extract_keypoints_from_frame(results):
    pose, lh, rh = [], [], []
    if results.pose_landmarks:
        for lm in results.pose_landmarks.landmark:
            pose.extend([lm.x, lm.y, lm.z, lm.visibility])
    else:
        pose = [0] * 132

    if results.left_hand_landmarks:
        for lm in results.left_hand_landmarks.landmark:
            lh.extend([lm.x, lm.y, lm.z])
    else:
        lh = [0] * 63

    if results.right_hand_landmarks:
        for lm in results.right_hand_landmarks.landmark:
            rh.extend([lm.x, lm.y, lm.z])
    else:
        rh = [0] * 63

    return np.array(pose + lh + rh)

def video_to_keypoints(video_path, max_frames=30):
    cap = cv2.VideoCapture(video_path)
    sequence = []
    with mp_holistic.Holistic(min_detection_confidence=0.5,
                              min_tracking_confidence=0.5) as holistic:
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = holistic.process(image)
            keypoints = extract_keypoints_from_frame(results)
            sequence.append(keypoints)
            if len(sequence) == max_frames:
                break
        cap.release()
    # pad sequence if shorter than 30 frames
    if len(sequence) < max_frames:
        sequence.extend([np.zeros(258)] * (max_frames - len(sequence)))
    return np.array(sequence)

video_path = "/content/drive/MyDrive/kaggle_datasets/indian_sign_language/Video_Dataset/Video_Dataset/Good Morning/WIN_20231103_13_50_28_Pro.mp4"

# Preprocess video
sequence = video_to_keypoints(video_path)
input_data = np.expand_dims(sequence, axis=0)  # shape (1, 30, 258)

# Predict
prediction = model.predict(input_data)
predicted_index = np.argmax(prediction)
predicted_word = actions[predicted_index]

print("ðŸ§  Predicted word:", predicted_word)
print("ðŸ”¢ Raw probabilities:", prediction)

top_indices = prediction[0].argsort()[-3:][::-1]
for i in top_indices:
    print(f"{actions[i]} â†’ {prediction[0][i]*100:.2f}%")