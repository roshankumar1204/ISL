# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1H3jWfdvJsnQNt-mo6bt3iQvgEl7wAD
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import kagglehub
import shutil
import os

# Step 1: Download dataset (will go into Colab's Kaggle cache)
path = kagglehub.dataset_download("prasadshet/indian-sign-language-video-dataset")
print("Dataset cached at:", path)

# Step 2: Copy dataset from cache into Google Drive
src = path
dst = "/content/drive/MyDrive/kaggle_datasets/indian_sign_language"

# Create target directory if not exists
os.makedirs(dst, exist_ok=True)

# Copy files/folders recursivelygrfcedxwq
shutil.copytree(src, dst, dirs_exist_ok=True)

print("âœ… Dataset copied to Google Drive at:", dst)

import os

# Path to your dataset inside Drive
base_path = "/content/drive/MyDrive/kaggle_datasets/indian_sign_language/Video_Dataset/Video_Dataset/"

# List all folders in the directory
all_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]

# Print only the first 3

print("First 3 folders:")
for folder in all_folders[:5]:
    print(folder)

import os
import random

# Base dataset path
DATA_PATH = "/content/drive/MyDrive/kaggle_datasets/indian_sign_language/Video_Dataset/Video_Dataset/"
# Get all available folders (words)
all_actions = [f for f in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, f))]

# Pick any 3 randomly
actions = random.sample(all_actions, 5)

print("ðŸ”Ž Total available words:", len(all_actions))
print("ðŸŽ¯ Selected words for training:", actions)

# Show number of videos inside each chosen folder
for action in actions:
    folder = os.path.join(DATA_PATH, action)
    print(f"ðŸ“‚ {action} â†’ {len(os.listdir(folder))} videos")



!pip install mediapipe opencv-python-headless

import cv2
import mediapipe as mp
import numpy as np

mp_holistic = mp.solutions.holistic

# Function to extract keypoints from one frame
def extract_keypoints_from_frame(results):
    pose, lh, rh = [], [], []

    # Pose (33 landmarks Ã— 4 values = 132)
    if results.pose_landmarks:
        for lm in results.pose_landmarks.landmark:
            pose.extend([lm.x, lm.y, lm.z, lm.visibility])
    else:
        pose = [0] * 132

    # Left hand (21 landmarks Ã— 3 values = 63)
    if results.left_hand_landmarks:
        for lm in results.left_hand_landmarks.landmark:
            lh.extend([lm.x, lm.y, lm.z])
    else:
        lh = [0] * 63

    # Right hand (21 landmarks Ã— 3 values = 63)
    if results.right_hand_landmarks:
        for lm in results.right_hand_landmarks.landmark:
            rh.extend([lm.x, lm.y, lm.z])
    else:
        rh = [0] * 63

    return np.array(pose + lh + rh)  # shape = 258


# Function to convert video â†’ sequence of keypoints
def video_to_keypoints(video_path, max_frames=30):
    cap = cv2.VideoCapture(video_path)
    sequence = []

    with mp_holistic.Holistic(min_detection_confidence=0.5,
                              min_tracking_confidence=0.5) as holistic:
        frame_count = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1

            # Convert BGR â†’ RGB
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = holistic.process(image)

            # Extract landmarks
            keypoints = extract_keypoints_from_frame(results)
            sequence.append(keypoints)

            # Stop if we reached max_frames
            if len(sequence) == max_frames:
                break

        cap.release()

    # Pad if sequence shorter
    if len(sequence) < max_frames:
        pad_length = max_frames - len(sequence)
        sequence.extend([np.zeros(sequence[0].shape)] * pad_length)

    print(f"ðŸ“¹ Processed {os.path.basename(video_path)} â†’ {frame_count} frames â†’ sequence shape {np.array(sequence).shape}")
    return np.array(sequence)

# test001

# Picking one sample video from first chosen action
sample_action = actions[0]
sample_video = os.listdir(os.path.join(DATA_PATH, sample_action))[0]
sample_path = os.path.join(DATA_PATH, sample_action, sample_video)

print("ðŸ”Ž Testing extraction on:", sample_path)
sequence = video_to_keypoints(sample_path)

print("âœ… Final sequence shape:", sequence.shape)  # should be (30, 258)

import numpy as np

X, y = [], []

print("ðŸ“‚ Starting dataset creation...\n")
for idx, action in enumerate(actions):
    folder = os.path.join(DATA_PATH, action)
    videos = os.listdir(folder)
    print(f"â–¶ Processing class: {action} ({len(videos)} videos)")

    for i, video in enumerate(videos):  # âš ï¸ for speed, limit to 10 videos first
        video_path = os.path.join(folder, video)
        sequence = video_to_keypoints(video_path)

        X.append(sequence)
        y.append(idx)

        print(f"   âœ… [{i+1}/{len(videos)}] {video} â†’ saved")

print("\nðŸŽ¯ Dataset creation complete.")
X = np.array(X)
y = np.array(y)

print("ðŸ”Ž Final shapes:")
print("   X:", X.shape)   # (num_samples, 30, 258)
print("   y:", y.shape)   # (num_samples,)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# One-hot encode labels
y_cat = tf.keras.utils.to_categorical(y, num_classes=len(actions))

print("ðŸ”Ž Data ready for training:")
print("   X shape:", X.shape)     # (samples, 30, 258)
print("   y shape:", y_cat.shape) # (samples, num_classes)

# Define model
model = Sequential([
    LSTM(64, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2])),
    LSTM(128, return_sequences=False, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(len(actions), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print("\nðŸ“Š Model Summary:")
model.summary()

# Train model
history = model.fit(
    X, y_cat,
    epochs=20,
    batch_size=8,
    validation_split=0.2,
    verbose=1
)

print("\nâœ… Training complete!")

model.save("/content/drive/MyDrive/isl_models/isl_sign_model.h5")

# Pick a random video from one of the chosen classes1
import random

test_action = random.choice(actions)
test_video = random.choice(os.listdir(os.path.join(DATA_PATH, test_action)))
test_path = os.path.join(DATA_PATH, test_action, test_video)

print(f"ðŸŽ¥ Testing on: {test_video} (actual label: {test_action})")

# Extract sequence for this video
test_seq = video_to_keypoints(test_path)
test_seq = np.expand_dims(test_seq, axis=0)  # shape (1, 30, 258)

# Make prediction
prediction = model.predict(test_seq)
pred_label = actions[np.argmax(prediction)]

print("ðŸ”® Model prediction:", pred_label)
print("ðŸ“Œ Actual label:", test_action)
print("ðŸ“Š Prediction probabilities:", prediction)

import matplotlib.pyplot as plt

# Plot Accuracy
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Model Accuracy")
plt.legend()
plt.show()

# Plot Loss
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Model Loss")
plt.legend()
plt.show()